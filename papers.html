<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
    <title>Knowledge and Logical Reasoning in the Era of Data-driven Learning | Workshop at ICML 2023</title>
    <meta name="generator" content="Jekyll v3.9.0" />
    <meta property="og:title" content="Knowledge and Logical Reasoning in the Era of Data-driven Learning" />
    <meta property="og:locale" content="en_US" />
    <meta name="description" content=" Workshop at ICML 2023" />
    <meta property="og:description" content="Workshop at ICML 2023" />
    <meta property="og:site_name" content="Knowledge and Logical Reasoning in the Era of Data-driven Learning" />
    <meta name="twitter:card" content="summary" />
    <meta property="twitter:title" content="Knowledge and Logical Reasoning in the Era of Data-driven Learning" />
    <script type="application/ld+json">
      {"headline":"Knowledge and Logical Reasoning in the Era of Data-driven Learning","url":"/papers.html","name":"Knowledge and Logical Reasoning in the Era of Data-driven Learning","description":"Workshop at ICML 2023","@type":"WebSite","@context":"https://schema.org"}</script>
    <!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
  <a id="skip-to-content" href="#content">Skip to the content.</a>

  <header class="page-header" role="banner">
    <h1 class="project-name">Knowledge and Logical Reasoning in the Era of Data-driven Learning</h1>
    <h2 class="project-tagline">Workshop at ICML 2023</h2>


    <a href="/" class="btn">Home</a>

    <a href="/cfp.html" class="btn">Call for Papers</a>

    <a href="/papers.html" class="btn">Accepted Papers</a>

    <a href="/schedule.html" class="btn">Schedule</a>

    <a href="/speakers.html" class="btn">Speakers</a>

    <a href="/organizers.html" class="btn">Organizers</a>

    <a href="/committee.html" class="btn">Program Committee</a>

    <!-- <a href="/related.html" class="btn">Related Workshops</a> -->

  </header>

    <main id="content" class="main-content" role="main">
      <h1 id="accepted-papers">Accepted Papers</h1>

      <h1 id="accepted-papers">Morning Session</h1>

      <ul>
        
        <li><b>SQA3D: Situated Question Answering in 3D Scenes</b> <br />
          Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, Siyuan Huang </li>
          
          
          <li><b>Retrieval-Augmented Multimodal Language Modeling</b> <br />
          Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih </li>
          
          
          <li><b>On the Aggregation of Rules for Knowledge Graph Completion</b> <br />
          Patrick Betz, Stefan Lüdtke, Christian Meilicke, Heiner Stuckenschmidt </li>
          
          
          <li><b>Large Language Model Programs</b> <br />
          Imanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz, Wen-tau Yih, Jason Weston, Jürgen Schmidhuber, Xian Li </li>
          
          
          <li><b>LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</b> <br />
          Kaiyu Yang, Aidan Swope, Alex Gu, Rahul R Chalamala, Shixing Yu, Saad Godil, Ryan Prenger, Animashree Anandkumar </li>
          
          
          <li><b>Semantically Adversarial Scene Generation with Explicit Knowledge Guidance for Autonomous Driving</b> <br />
          Wenhao Ding, Haohong Lin, Bo Li, Ding Zhao </li>
          
          
          <li><b>Towards true discovery of the differential equations</b> <br />
          Alexander Hvatov, Roman Titov </li>
        
          <li><b>Neural Priority Queues for GNNs</b> <br />
          Rishabh Jain, Petar Veličković, Pietro Lió </li>
          
          
          <li><b>VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming</b> <br />
          Eleonora Misino, Giuseppe Marra, Emanuele Sansone </li>
          
          
          <li><b>Explanatory Learning: Towards Artificial Scientific Discovery</b> <br />
          Antonio Norelli, Giorgio Mariani, Luca Moschella, Andrea Santilli, Giambattista Parascandolo, Simone Melzi, Emanuele Rodola </li>
          
          
          <li><b>ANet: A Scalable Path-based Reasoning Approach for Knowledge Graphs</b> <br />
          Zhaocheng Zhu, XINYU YUAN, Mikhail Galkin, Louis-Pascal Xhonneux, Ming Zhang, Maxime Gazeau, Jian Tang </li>
          
          
          <li><b>DiversiGATE: A Comprehensive Framework for Reliable Large Language Models</b> <br />
          Shima Imani, Ali Beyram, Harsh Shrivastava </li>

          <li><b>Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal</b><br />
          Emanuele Marconato, Gianpaolo Bontempo, Elisa Ficarra, SIMONE CALDERARA, Andrea Passerini, Stefano Teso </li>
        
          
          
          <li><b>OC-NMN: Object-centric Compositional Neural Module Network for Generative Visual Analogical Reasoning</b> <br />
          Rim Assouel, Pau Rodriguez, Perouz Taslakian, David Vazquez, Yoshua Bengio </li>
          
          
          <li><b>Look, Remember and Reason: Visual Reasoning with Grounded Rationales</b> <br />
          Apratim Bhattacharyya, Sunny P Panchal, Mingu Lee, Reza Pourreza, Pulkit Madan, Roland Memisevic </li>
          
          
          <li><b>Describe, Explain, Plan and Select: Interactive Planning with LLMs Enables Open-World Multi-Task Agents</b> <br />
          Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang </li>
          
          
          <li><b>Recursive Algorithmic Reasoning</b> <br />
          Dulhan Jayalath, Jonas Jürß, Petar Veličković </li>
          
          
          <li><b>EXPLAIN, AGREE and LEARN: A Recipe for Scalable Neural-Symbolic Learning</b> <br />
          Victor Verreet, Lennert De Smet, Emanuele Sansone </li>
          
          
          <li><b>Semantic Conditioning at Inference : Improving Neural-based Systems with Logical Background Knowledge</b> <br />
          Arthur Ledaguenel, Céline Hudelot, Mostepha Khouadjia </li>
          
          
          <li><b>Continuous-Discrete Message Passing for Graph Logic Reasoning</b> <br />
          Cristóbal Corvalán Morbiducci, Francesco Alesiani, Markus Zopf </li>
          
          
          <li><b>Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples</b> <br />
          Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He </li>
          
          
          <li><b>Evidence of Meaning in Language Models Trained on Programs</b> <br />
          Charles Jin, Martin Rinard </li>
          
          
          <li><b>Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs</b> <br />
          Lauren Nicole DeLong, Ramon Fernández Mir, Zonglin Ji, Fiona Niamh Coulter Smith, Jacques D. Fleuriot </li>
          
          <li><b>Bayesian Neural Networks with Domain Knowledge</b> <br />
          Dylan Sam, Rattana Pukdee, Daniel P Jeong, Yewon Byun, Zico Kolter </li>
          
          <li><b>Exposing Attention Glitches with Flip-Flop Language Modeling</b> <br />
          Bingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang </li>
          
          
          <li><b>Does End-to-End Visual Pretraining Help Reasoning?</b> <br />
          Chen Sun, Calvin Luo, Xingyi Zhou, Anurag Arnab, Cordelia Schmid </li>
          
          
          <li><b>On the Planning Abilities of Large Language Models - A Critical Investigation</b> <br />
          Karthik Valmeekam, Matthew D Marquez, Sarath Sreedharan, Subbarao Kambhampati </li>
          
          
          <li><b>Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning</b> <br />
          Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati </li>
          
          
          <li><b>On The Ability of Transformers To Learn Recursive Patterns</b> <br />
          Dylan Zhang, Curt Tigges, Talia Ringer, Stella Biderman, Maxim Raginsky </li>
          
          
          <li><b>Reasoning Ability Emerges in Large Language Models as Aggregation of Reasoning Paths</b> <br />
          Xinyi Wang, William Yang Wang </li>
          
          
          <li><b>Exploring the Impact of Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning</b> <br />
          Roshanak Mirzaee, Parisa Kordjamshidi </li>
          
          
          <li><b>Plan, Eliminate, and Track --- Language Models are Good Teachers for Embodied Agents.</b> <br />
          Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, Shrimai Prabhumoye </li>
          
          
          <li><b>SPRING: Studying Papers and Reasoning to play Games</b> <br />
          Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li </li>
      </ul>ul>
           <h1 id="accepted-papers">Afternoon Session</h1>
          <ul>
          <li><b>Parallel Algorithms Align with Neural Execution</b> <br />
          Valerie Engelmayer, Dobrik G. Georgiev, Petar Veličković </li>
          
          
          <li><b>Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models</b> <br />
          Daman Arora, Subbarao Kambhampati </li>
          
          
          <li><b>Latent Space Representations of Neural Algorithmic Reasoners</b> <br />
          Viktor Mirjanic, Razvan Pascanu, Petar Veličković </li>
          
          
          <li><b>Towards More Likely Models for AI Planning</b> <br />
          Turgay Caglar, sirine belhaj, Tathagata Chakraborti, Michael Katz, Sarath Sreedharan </li>
          
          
          <li><b>A Pseudo-Semantic Loss for Deep Generative Models with Logical Constraints</b> <br />
          Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck </li>
          
          
          <li><b>Asynchronous Algorithmic Alignment with Cocycles</b> <br />
          Andrew J Dudzik, Tamara von Glehn, Razvan Pascanu, Petar Veličković </li>
          
          
          <li><b>Learning with Explanation Constraints</b> <br />
          Rattana Pukdee, Dylan Sam, Maria-Florina Balcan, Pradeep Ravikumar </li>
          
          
          <li><b>BoardgameQA: Natural Language Reasoning with Contradictory Information</b> <br />
          Mehran Kazemi, Quan Yuan, DEEPTI BHATIA, Najoung Kim, Xin Xu, Vaiva Imbrasaite, Deepak Ramachandran </li>
          
          
          <li><b>Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting</b> <br />
          Rylan Schaeffer, Kateryna Pistunova, Samar Khanna, Sarthak Consul, Sanmi Koyejo </li>
          
          
          <li><b>interpretability of Transformers: a case study with Dyck grammars</b> <br />
          Kaiyue Wen, Yuchen Li, Bingbin Liu, Andrej Risteski </li>
          
          
          <li><b>dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning</b> <br />
          Renato L Geh, Jonas L Goncalves, Igor Silveira, Denis D Maua, Fabio Cozman </li>
          
          
          <li><b>Building One-class Detector for Anything: Open-vocabulary Zero-shot OOD Detection Using Text-image Models</b> <br />
          Yunhao Ge, Jie Ren, Jiaping Zhao, Kaifeng Chen, Andrew Gallagher, Laurent Itti, Balaji Lakshminarayanan </li>
          
          
          <li><b>Disaster Occurrence Detection through GNN Models using Disaster Knowledge Graphs</b> <br />
          Seonhyeong Kim, Irshad Khan, Young-Woo Kwon </li>
          
          
          <li><b>Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</b> <br />
          Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma </li>
          
          
          <li><b>Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting</b> <br />
          Hejie Cui, Xinyu Fang, Zihan Zhang, Ran Xu, Xuan Kan, Xin Liu, Manling Li, Yangqiu Song, Carl Yang </li>
          
          
          <li><b>Towards A Unified Neural Architecture for Visual Recognition and Reasoning</b> <br />
          Calvin Luo, Boqing Gong, Ting Chen, Chen Sun </li>
          
          
          <li><b>How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding</b> <br />
          Yuchen Li, Yuanzhi Li, Andrej Risteski </li>
          
          
          <li><b>Large Language Models are Zero-Shot Multi-Tool Users</b> <br />
          Luca Beurer-Kellner, Marc Fischer, Martin Vechev </li>
          
          
          <li><b>Training LLMs with Noisy Algorithmic Chain of Thought</b> <br />
          Alexander Havrilla </li>
          
          
          <li><b>The Role of Semantic Parsing in Understanding Procedural Text</b> <br />
          Hossein Rajaby Faghihi, Parisa Kordjamshidi, Choh Man Teng, James Allen </li>
          
          
          <li><b>Partial Label Learning meets Active Learning: Enhancing Annotation Efficiency through Binary Questioning</b> <br />
          Shivangana Rawat, Chaitanya Devaguptapu, Vineeth Balasubramanian </li>
          
          
          <li><b>Learning to Initiate and Reason in Event-Driven Cascading Processes</b> <br />
          Yuval Atzmon, eli meirom, Shie Mannor, Gal Chechik </li>
          
          
          <li><b>LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models</b> <br />
          Long Lian, Boyi Li, Adam Yala, Trevor Darrell </li>
          
          
          <li><b>Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning</b> <br />
          Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Y Zhang, Jun Zhou, Chenhao Tan, Hongyuan Mei </li>
          
          
          <li><b>What’s left can’t be right - The remaining positional incompetence of contrastive vision-language models</b> <br />
          Nils Höhing, Ellen Rushe, Anthony Ventresque </li>
          
          
          <li><b>Deep Neuro-Symbolic Weight Learning in Neural Probabilistic Soft Logic</b> <br />
          Connor Pryor, Charles Dickens, Lise Getoor </li>
          
          
          <li><b>Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks</b> <br />
          Savannah Thais, Daniel Murnane </li>
          
          
          <li><b>Revealing the Intrinsic Ability of Generative Language Models in Relation Prediction</b> <br />
          Qi Li, Lyuwen Wu, Luoyi Fu, Xinbing Wang, Lei Zhou, Chenghu Zhou, Shiyu Liang </li>
          
          
          <li><b>Augmenting the Knowledge to Large Model from Federated Small Models</b> <br />
          Miru Kim, Minhae Kwon </li>
          
          
          <li><b>Explicit Planning Helps Language Models in Logical Reasoning</b> <br />
          Hongyu Zhao, Kangrui Wang, Mo Yu, Hongyuan Mei </li>
          
          
          <li><b>Evaluating the Casual Reasoning Abilities of Large Language Models</b> <br />
          Isha Puri, Himabindu Lakkaraju </li>
      </ul>




<footer class="site-footer">
  <span class="site-footer-credits">Please contact <a href="mailto:nezihe.guerel@inf.ethz.ch">Nezihe Merve Gürel</a>  or <a href="mailto:lbo@illinois.edu">Bo Li</a> if you have any questions.<br> The webpage template is by the courtesy of <a href="https://tda-in-ml.github.io/">NeurIPS 2020 Workshop on Topological Data Analysis and Beyond</a>. <br>

  </span>
  </footer>
    </main>
  </body>
</html>
